{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RunClicr.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8_80LtZtaSa",
        "colab_type": "code",
        "outputId": "cd5a46c4-423b-4116-e9aa-e66a7d5ba046",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/'My Drive'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n",
            "/gdrive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFFYT7mAtbeh",
        "colab_type": "code",
        "outputId": "b7c205cd-2bbd-47cf-c4f8-fbd9a914c2c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# coding=utf-8\n",
        "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
        "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\" Finetuning the library models for question-answering on SQuAD (DistilBERT, Bert, XLM, XLNet).\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Finetuning the library models for question-answering on SQuAD (DistilBERT, Bert, XLM, XLNet).'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AF8lGKT3tdIl",
        "colab_type": "code",
        "outputId": "1527eef1-968e-4b96-bf50-d5d1caf31f46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        }
      },
      "source": [
        "!pip install sentencepiece\n",
        "!pip install tokenizers\n",
        "!pip install sacremoses\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\r\u001b[K     |▎                               | 10kB 20.8MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 4.8MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 6.9MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40kB 8.7MB/s eta 0:00:01\r\u001b[K     |█▋                              | 51kB 5.4MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 6.4MB/s eta 0:00:01\r\u001b[K     |██▏                             | 71kB 7.0MB/s eta 0:00:01\r\u001b[K     |██▌                             | 81kB 7.9MB/s eta 0:00:01\r\u001b[K     |██▉                             | 92kB 8.7MB/s eta 0:00:01\r\u001b[K     |███▏                            | 102kB 7.1MB/s eta 0:00:01\r\u001b[K     |███▌                            | 112kB 7.1MB/s eta 0:00:01\r\u001b[K     |███▉                            | 122kB 7.1MB/s eta 0:00:01\r\u001b[K     |████                            | 133kB 7.1MB/s eta 0:00:01\r\u001b[K     |████▍                           | 143kB 7.1MB/s eta 0:00:01\r\u001b[K     |████▊                           | 153kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████                           | 163kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 174kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 184kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████                          | 194kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 204kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 215kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████                         | 225kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 235kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 245kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 256kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 266kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 276kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 286kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 296kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 307kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 317kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████                      | 327kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 337kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 348kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████                     | 358kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 368kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 378kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████                    | 389kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 399kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 409kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 419kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 430kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 440kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 450kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 460kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 471kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 481kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 491kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 501kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 512kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████                | 522kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 532kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 542kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 552kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 563kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 573kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 583kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 593kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 604kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 614kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 624kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 634kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 645kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 655kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 665kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 675kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 686kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 696kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 706kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 716kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 727kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 737kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 747kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 757kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 768kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 778kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 788kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 798kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 808kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 819kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 829kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 839kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 849kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 860kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 870kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 880kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 890kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 901kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 911kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 921kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 931kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 942kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 952kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 962kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 972kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 983kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 993kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.0MB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.0MB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.0MB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.0MB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.0MB 7.1MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.85\n",
            "Collecting tokenizers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/de/ec55e2d5a8720557b25100dd7dd4a63108a44b6b303978ce2587666931cf/tokenizers-0.6.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 7.0MB/s \n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.6.0\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 7.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from sacremoses) (2019.12.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses) (7.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses) (0.14.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from sacremoses) (4.38.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=a0afc4ef764ccd231afedb7bb0674920e569f880a0d0f5a6a1a9cf569a1d51bb\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses\n",
            "Successfully installed sacremoses-0.0.38\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02nSXt4tte2_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import argparse\n",
        "import glob\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "import pprint\n",
        "import timeit\n",
        "import sys\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from tqdm import tqdm, trange\n",
        "sys.path.append('CSE576/data/Output/')\n",
        "sys.path.append('CSE576/transformers-master/src/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuY-oioFtgnD",
        "colab_type": "code",
        "outputId": "de70bbfb-1c33-4250-eefb-9f8ca874991f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 583
        }
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 8428795733152639741\n",
            ", name: \"/device:XLA_CPU:0\"\n",
            "device_type: \"XLA_CPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 15960488903049648572\n",
            "physical_device_desc: \"device: XLA_CPU device\"\n",
            ", name: \"/device:XLA_GPU:0\"\n",
            "device_type: \"XLA_GPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 4017441386187413075\n",
            "physical_device_desc: \"device: XLA_GPU device\"\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 15505902797\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 485263062951220937\n",
            "physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AGr5f3-tiUu",
        "colab_type": "code",
        "outputId": "65bf26be-0b13-44e1-ae35-801f8ba1a57d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Mar 23 20:27:51 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P0    36W / 250W |    717MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zUX4whbbAlN",
        "colab_type": "code",
        "outputId": "81a04c52-5e26-44d0-e74f-5b913deb13aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "import pprint\n",
        "import tensorflow as tf\n",
        "\n",
        "if 'COLAB_TPU_ADDR' not in os.environ:\n",
        "  print('ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!')\n",
        "else:\n",
        "  tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "  print ('TPU address is', tpu_address)\n",
        "\n",
        "  with tf.Session(tpu_address) as session:\n",
        "    devices = session.list_devices()\n",
        "    \n",
        "  print('TPU devices:')\n",
        "  pprint.pprint(devices)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgYCESKAtkRm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import (\n",
        "    \n",
        "    WEIGHTS_NAME,\n",
        "    AdamW,\n",
        "    AlbertConfig,\n",
        "    AlbertForQuestionAnswering,\n",
        "    AlbertTokenizer,\n",
        "    BertConfig,\n",
        "    BertForQuestionAnswering,\n",
        "    BertTokenizer,\n",
        "    CamembertConfig,\n",
        "    CamembertForQuestionAnswering,\n",
        "    CamembertTokenizer,\n",
        "    DistilBertConfig,\n",
        "    DistilBertForQuestionAnswering,\n",
        "    DistilBertTokenizer,\n",
        "    RobertaConfig,\n",
        "    RobertaForQuestionAnswering,\n",
        "    RobertaTokenizer,\n",
        "    XLMConfig,\n",
        "    XLMForQuestionAnswering,\n",
        "    XLMTokenizer,\n",
        "    XLNetConfig,\n",
        "    XLNetForQuestionAnswering,\n",
        "    XLNetTokenizer,\n",
        "    get_linear_schedule_with_warmup,\n",
        "    squad_convert_examples_to_features,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPipX5Gltl3j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers.data.metrics.squad_metrics import (\n",
        "    compute_predictions_log_probs,\n",
        "    compute_predictions_logits,\n",
        "    squad_evaluate,\n",
        ")\n",
        "from transformers.data.processors.squad import SquadResult, SquadV1Processor, SquadV2Processor\n",
        "\n",
        "\n",
        "try:\n",
        "    from torch.utils.tensorboard import SummaryWriter\n",
        "except ImportError:\n",
        "    from tensorboardX import SummaryWriter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KEFqjbmtnZ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "ALL_MODELS = sum(\n",
        "    (\n",
        "        tuple(conf.pretrained_config_archive_map.keys())\n",
        "        for conf in (BertConfig, CamembertConfig, RobertaConfig, XLNetConfig, XLMConfig)\n",
        "    ),\n",
        "    (),\n",
        ")\n",
        "\n",
        "MODEL_CLASSES = {\n",
        "    \"bert\": (BertConfig, BertForQuestionAnswering, BertTokenizer),\n",
        "    \"camembert\": (CamembertConfig, CamembertForQuestionAnswering, CamembertTokenizer),\n",
        "    \"roberta\": (RobertaConfig, RobertaForQuestionAnswering, RobertaTokenizer),\n",
        "    \"xlnet\": (XLNetConfig, XLNetForQuestionAnswering, XLNetTokenizer),\n",
        "    \"xlm\": (XLMConfig, XLMForQuestionAnswering, XLMTokenizer),\n",
        "    \"distilbert\": (DistilBertConfig, DistilBertForQuestionAnswering, DistilBertTokenizer),\n",
        "    \"albert\": (AlbertConfig, AlbertForQuestionAnswering, AlbertTokenizer),\n",
        "}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZh1F7L1to5q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def set_seed(args):\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    if args.n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "\n",
        "def to_list(tensor):\n",
        "    return tensor.detach().cpu().tolist()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hi04Ek5CtrXO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(args, train_dataset, model, tokenizer):\n",
        "    \"\"\" Train the model \"\"\"\n",
        "    if args.local_rank in [-1, 0]:\n",
        "        tb_writer = SummaryWriter()\n",
        "\n",
        "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
        "    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
        "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n",
        "\n",
        "    if args.max_steps > 0:\n",
        "        t_total = args.max_steps\n",
        "        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
        "    else:\n",
        "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
        "\n",
        "    # Prepare optimizer and schedule (linear warmup and decay)\n",
        "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\n",
        "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "            \"weight_decay\": args.weight_decay,\n",
        "        },\n",
        "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
        "    ]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
        "    )\n",
        "\n",
        "    # Check if saved optimizer or scheduler states exist\n",
        "    if os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\")) and os.path.isfile(\n",
        "        os.path.join(args.model_name_or_path, \"scheduler.pt\")\n",
        "    ):\n",
        "        # Load in optimizer and scheduler states\n",
        "        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n",
        "        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n",
        "\n",
        "    if args.fp16:\n",
        "        try:\n",
        "            from apex import amp\n",
        "        except ImportError:\n",
        "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
        "\n",
        "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
        "\n",
        "    # multi-gpu training (should be after apex fp16 initialization)\n",
        "    if args.n_gpu > 1:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # Distributed training (should be after apex fp16 initialization)\n",
        "    if args.local_rank != -1:\n",
        "        model = torch.nn.parallel.DistributedDataParallel(\n",
        "            model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True\n",
        "        )\n",
        "\n",
        "    # Train!\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
        "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
        "    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
        "    logger.info(\n",
        "        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
        "        args.train_batch_size\n",
        "        * args.gradient_accumulation_steps\n",
        "        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n",
        "    )\n",
        "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
        "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
        "\n",
        "    global_step = 1\n",
        "    epochs_trained = 0\n",
        "    steps_trained_in_current_epoch = 0\n",
        "    # Check if continuing training from a checkpoint\n",
        "    if os.path.exists(args.model_name_or_path):\n",
        "        try:\n",
        "            # set global_step to gobal_step of last saved checkpoint from model path\n",
        "            checkpoint_suffix = args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0]\n",
        "            global_step = int(checkpoint_suffix)\n",
        "            epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n",
        "            steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n",
        "\n",
        "            logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n",
        "            logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n",
        "            logger.info(\"  Continuing training from global step %d\", global_step)\n",
        "            logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n",
        "        except ValueError:\n",
        "            logger.info(\"  Starting fine-tuning.\")\n",
        "\n",
        "    tr_loss, logging_loss = 0.0, 0.0\n",
        "    model.zero_grad()\n",
        "    train_iterator = trange(\n",
        "        epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n",
        "    )\n",
        "    # Added here for reproductibility\n",
        "    set_seed(args)\n",
        "\n",
        "    for _ in train_iterator:\n",
        "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n",
        "        for step, batch in enumerate(epoch_iterator):\n",
        "\n",
        "            # Skip past any already trained steps if resuming training\n",
        "            if steps_trained_in_current_epoch > 0:\n",
        "                steps_trained_in_current_epoch -= 1\n",
        "                continue\n",
        "\n",
        "            model.train()\n",
        "            batch = tuple(t.to(args.device) for t in batch)\n",
        "\n",
        "            inputs = {\n",
        "                \"input_ids\": batch[0],\n",
        "                \"attention_mask\": batch[1],\n",
        "                \"token_type_ids\": batch[2],\n",
        "                \"start_positions\": batch[3],\n",
        "                \"end_positions\": batch[4],\n",
        "            }\n",
        "\n",
        "            if args.model_type in [\"xlm\", \"roberta\", \"distilbert\", \"camembert\"]:\n",
        "                del inputs[\"token_type_ids\"]\n",
        "\n",
        "            if args.model_type in [\"xlnet\", \"xlm\"]:\n",
        "                inputs.update({\"cls_index\": batch[5], \"p_mask\": batch[6]})\n",
        "                if args.version_2_with_negative:\n",
        "                    inputs.update({\"is_impossible\": batch[7]})\n",
        "                if hasattr(model, \"config\") and hasattr(model.config, \"lang2id\"):\n",
        "                    inputs.update(\n",
        "                        {\"langs\": (torch.ones(batch[0].shape, dtype=torch.int64) * args.lang_id).to(args.device)}\n",
        "                    )\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            # model outputs are always tuple in transformers (see doc)\n",
        "            loss = outputs[0]\n",
        "\n",
        "            if args.n_gpu > 1:\n",
        "                loss = loss.mean()  # mean() to average on multi-gpu parallel (not distributed) training\n",
        "            if args.gradient_accumulation_steps > 1:\n",
        "                loss = loss / args.gradient_accumulation_steps\n",
        "\n",
        "            if args.fp16:\n",
        "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                    scaled_loss.backward()\n",
        "            else:\n",
        "                loss.backward()\n",
        "\n",
        "            tr_loss += loss.item()\n",
        "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
        "                if args.fp16:\n",
        "                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
        "                else:\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "\n",
        "                optimizer.step()\n",
        "                scheduler.step()  # Update learning rate schedule\n",
        "                model.zero_grad()\n",
        "                global_step += 1\n",
        "\n",
        "                # Log metrics\n",
        "                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
        "                    # Only evaluate when single GPU otherwise metrics may not average well\n",
        "                    if args.local_rank == -1 and args.evaluate_during_training:\n",
        "                        results = evaluate(args, model, tokenizer)\n",
        "                        for key, value in results.items():\n",
        "                            tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n",
        "                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n",
        "                    tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n",
        "                    logging_loss = tr_loss\n",
        "\n",
        "                # Save model checkpoint\n",
        "                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n",
        "                    output_dir = os.path.join(args.output_dir, \"checkpoint-{}\".format(global_step))\n",
        "                    if not os.path.exists(output_dir):\n",
        "                        os.makedirs(output_dir)\n",
        "                    # Take care of distributed/parallel training\n",
        "                    model_to_save = model.module if hasattr(model, \"module\") else model\n",
        "                    model_to_save.save_pretrained(output_dir)\n",
        "                    tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n",
        "                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
        "\n",
        "                    torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
        "                    torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
        "                    logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n",
        "\n",
        "            if args.max_steps > 0 and global_step > args.max_steps:\n",
        "                epoch_iterator.close()\n",
        "                break\n",
        "        if args.max_steps > 0 and global_step > args.max_steps:\n",
        "            train_iterator.close()\n",
        "            break\n",
        "\n",
        "    if args.local_rank in [-1, 0]:\n",
        "        tb_writer.close()\n",
        "\n",
        "    return global_step, tr_loss / global_step"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gj-_36mxtuHg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(args, model, tokenizer, prefix=\"\"):\n",
        "    dataset, examples, features = load_and_cache_examples(args, tokenizer, evaluate=True, output_examples=True)\n",
        "\n",
        "    if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:\n",
        "        os.makedirs(args.output_dir)\n",
        "\n",
        "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
        "\n",
        "    # Note that DistributedSampler samples randomly\n",
        "    eval_sampler = SequentialSampler(dataset)\n",
        "    eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
        "\n",
        "    # multi-gpu evaluate\n",
        "    if args.n_gpu > 1 and not isinstance(model, torch.nn.DataParallel):\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # Eval!\n",
        "    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
        "    logger.info(\"  Num examples = %d\", len(dataset))\n",
        "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
        "\n",
        "    all_results = []\n",
        "    start_time = timeit.default_timer()\n",
        "\n",
        "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "        model.eval()\n",
        "        batch = tuple(t.to(args.device) for t in batch)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            inputs = {\n",
        "                \"input_ids\": batch[0],\n",
        "                \"attention_mask\": batch[1],\n",
        "                \"token_type_ids\": batch[2],\n",
        "            }\n",
        "\n",
        "            if args.model_type in [\"xlm\", \"roberta\", \"distilbert\", \"camembert\"]:\n",
        "                del inputs[\"token_type_ids\"]\n",
        "\n",
        "            example_indices = batch[3]\n",
        "\n",
        "            # XLNet and XLM use more arguments for their predictions\n",
        "            if args.model_type in [\"xlnet\", \"xlm\"]:\n",
        "                inputs.update({\"cls_index\": batch[4], \"p_mask\": batch[5]})\n",
        "                # for lang_id-sensitive xlm models\n",
        "                if hasattr(model, \"config\") and hasattr(model.config, \"lang2id\"):\n",
        "                    inputs.update(\n",
        "                        {\"langs\": (torch.ones(batch[0].shape, dtype=torch.int64) * args.lang_id).to(args.device)}\n",
        "                    )\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "        for i, example_index in enumerate(example_indices):\n",
        "            eval_feature = features[example_index.item()]\n",
        "            unique_id = int(eval_feature.unique_id)\n",
        "\n",
        "            output = [to_list(output[i]) for output in outputs]\n",
        "\n",
        "            # Some models (XLNet, XLM) use 5 arguments for their predictions, while the other \"simpler\"\n",
        "            # models only use two.\n",
        "            if len(output) >= 5:\n",
        "                start_logits = output[0]\n",
        "                start_top_index = output[1]\n",
        "                end_logits = output[2]\n",
        "                end_top_index = output[3]\n",
        "                cls_logits = output[4]\n",
        "\n",
        "                result = SquadResult(\n",
        "                    unique_id,\n",
        "                    start_logits,\n",
        "                    end_logits,\n",
        "                    start_top_index=start_top_index,\n",
        "                    end_top_index=end_top_index,\n",
        "                    cls_logits=cls_logits,\n",
        "                )\n",
        "\n",
        "            else:\n",
        "                start_logits, end_logits = output\n",
        "                result = SquadResult(unique_id, start_logits, end_logits)\n",
        "\n",
        "            all_results.append(result)\n",
        "\n",
        "    evalTime = timeit.default_timer() - start_time\n",
        "    logger.info(\"  Evaluation done in total %f secs (%f sec per example)\", evalTime, evalTime / len(dataset))\n",
        "\n",
        "    # Compute predictions\n",
        "    output_prediction_file = os.path.join(args.output_dir, \"predictions_{}.json\".format(prefix))\n",
        "    output_nbest_file = os.path.join(args.output_dir, \"nbest_predictions_{}.json\".format(prefix))\n",
        "\n",
        "    if args.version_2_with_negative:\n",
        "        output_null_log_odds_file = os.path.join(args.output_dir, \"null_odds_{}.json\".format(prefix))\n",
        "    else:\n",
        "        output_null_log_odds_file = None\n",
        "\n",
        "    # XLNet and XLM use a more complex post-processing procedure\n",
        "    if args.model_type in [\"xlnet\", \"xlm\"]:\n",
        "        start_n_top = model.config.start_n_top if hasattr(model, \"config\") else model.module.config.start_n_top\n",
        "        end_n_top = model.config.end_n_top if hasattr(model, \"config\") else model.module.config.end_n_top\n",
        "\n",
        "        predictions = compute_predictions_log_probs(\n",
        "            examples,\n",
        "            features,\n",
        "            all_results,\n",
        "            args.n_best_size,\n",
        "            args.max_answer_length,\n",
        "            output_prediction_file,\n",
        "            output_nbest_file,\n",
        "            output_null_log_odds_file,\n",
        "            start_n_top,\n",
        "            end_n_top,\n",
        "            args.version_2_with_negative,\n",
        "            tokenizer,\n",
        "            args.verbose_logging,\n",
        "        )\n",
        "    else:\n",
        "        predictions = compute_predictions_logits(\n",
        "            examples,\n",
        "            features,\n",
        "            all_results,\n",
        "            args.n_best_size,\n",
        "            args.max_answer_length,\n",
        "            args.do_lower_case,\n",
        "            output_prediction_file,\n",
        "            output_nbest_file,\n",
        "            output_null_log_odds_file,\n",
        "            args.verbose_logging,\n",
        "            args.version_2_with_negative,\n",
        "            args.null_score_diff_threshold,\n",
        "            tokenizer,\n",
        "        )\n",
        "\n",
        "    # Compute the F1 and exact scores.\n",
        "    results = squad_evaluate(examples, predictions)\n",
        "    return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6flMMdmtvyZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_and_cache_examples(args, tokenizer, evaluate=False, output_examples=False):\n",
        "    if args.local_rank not in [-1, 0] and not evaluate:\n",
        "        # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
        "        torch.distributed.barrier()\n",
        "\n",
        "    # Load data features from cache or dataset file\n",
        "    input_dir = args.data_dir if args.data_dir else \".\"\n",
        "    cached_features_file = os.path.join(\n",
        "        input_dir,\n",
        "        \"cached_{}_{}_{}\".format(\n",
        "            \"dev\" if evaluate else \"train\",\n",
        "            list(filter(None, args.model_name_or_path.split(\"/\"))).pop(),\n",
        "            str(args.max_seq_length),\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # Init features and dataset from cache if it exists\n",
        "    if os.path.exists(cached_features_file) and not args.overwrite_cache:\n",
        "        logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
        "        features_and_dataset = torch.load(cached_features_file)\n",
        "        features, dataset, examples = (\n",
        "            features_and_dataset[\"features\"],\n",
        "            features_and_dataset[\"dataset\"],\n",
        "            features_and_dataset[\"examples\"],\n",
        "        )\n",
        "    else:\n",
        "        logger.info(\"Creating features from dataset file at %s\", input_dir)\n",
        "\n",
        "        if not args.data_dir and ((evaluate and not args.predict_file) or (not evaluate and not args.train_file)):\n",
        "            try:\n",
        "                import tensorflow_datasets as tfds\n",
        "            except ImportError:\n",
        "                raise ImportError(\"If not data_dir is specified, tensorflow_datasets needs to be installed.\")\n",
        "\n",
        "            if args.version_2_with_negative:\n",
        "                logger.warn(\"tensorflow_datasets does not handle version 2 of SQuAD.\")\n",
        "\n",
        "            tfds_examples = tfds.load(\"squad\")\n",
        "            examples = SquadV1Processor().get_examples_from_dataset(tfds_examples, evaluate=evaluate)\n",
        "        else:\n",
        "            processor = SquadV2Processor() if args.version_2_with_negative else SquadV1Processor()\n",
        "            if evaluate:\n",
        "                examples = processor.get_dev_examples(args.data_dir, filename=args.predict_file)\n",
        "            else:\n",
        "                examples = processor.get_train_examples(args.data_dir, filename=args.train_file)\n",
        "\n",
        "        features, dataset = squad_convert_examples_to_features(\n",
        "            examples=examples,\n",
        "            tokenizer=tokenizer,\n",
        "            max_seq_length=args.max_seq_length,\n",
        "            doc_stride=args.doc_stride,\n",
        "            max_query_length=args.max_query_length,\n",
        "            is_training=not evaluate,\n",
        "            return_dataset=\"pt\",\n",
        "            threads=args.threads,\n",
        "        )\n",
        "\n",
        "        if args.local_rank in [-1, 0]:\n",
        "            logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
        "            torch.save({\"features\": features, \"dataset\": dataset, \"examples\": examples}, cached_features_file)\n",
        "\n",
        "    if args.local_rank == 0 and not evaluate:\n",
        "        # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
        "        torch.distributed.barrier()\n",
        "\n",
        "    if output_examples:\n",
        "        return dataset, examples, features\n",
        "    return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Akxk1VJtxfO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # Required parameters\n",
        "    parser.add_argument(\n",
        "        \"--model_type\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        required=True,\n",
        "        help=\"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.keys()),\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--model_name_or_path\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        required=True,\n",
        "        help=\"Path to pre-trained model or shortcut name selected in the list: \" + \", \".join(ALL_MODELS),\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--output_dir\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        required=True,\n",
        "        help=\"The output directory where the model checkpoints and predictions will be written.\",\n",
        "    )\n",
        "\n",
        "    # Other parameters\n",
        "    parser.add_argument(\n",
        "        \"--data_dir\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        help=\"The input data dir. Should contain the .json files for the task.\"\n",
        "        + \"If no data dir or train/predict files are specified, will run with tensorflow_datasets.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--train_file\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        help=\"The input training file. If a data dir is specified, will look for the file there\"\n",
        "        + \"If no data dir or train/predict files are specified, will run with tensorflow_datasets.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--predict_file\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        help=\"The input evaluation file. If a data dir is specified, will look for the file there\"\n",
        "        + \"If no data dir or train/predict files are specified, will run with tensorflow_datasets.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--config_name\", default=\"\", type=str, help=\"Pretrained config name or path if not the same as model_name\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--tokenizer_name\",\n",
        "        default=\"\",\n",
        "        type=str,\n",
        "        help=\"Pretrained tokenizer name or path if not the same as model_name\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--cache_dir\",\n",
        "        default=\"\",\n",
        "        type=str,\n",
        "        help=\"Where do you want to store the pre-trained models downloaded from s3\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--version_2_with_negative\",\n",
        "        action=\"store_true\",\n",
        "        help=\"If true, the SQuAD examples contain some that do not have an answer.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--null_score_diff_threshold\",\n",
        "        type=float,\n",
        "        default=0.0,\n",
        "        help=\"If null_score - best_non_null is greater than the threshold predict null.\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--max_seq_length\",\n",
        "        default=384,\n",
        "        type=int,\n",
        "        help=\"The maximum total input sequence length after WordPiece tokenization. Sequences \"\n",
        "        \"longer than this will be truncated, and sequences shorter than this will be padded.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--doc_stride\",\n",
        "        default=128,\n",
        "        type=int,\n",
        "        help=\"When splitting up a long document into chunks, how much stride to take between chunks.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--max_query_length\",\n",
        "        default=64,\n",
        "        type=int,\n",
        "        help=\"The maximum number of tokens for the question. Questions longer than this will \"\n",
        "        \"be truncated to this length.\",\n",
        "    )\n",
        "    parser.add_argument(\"--do_train\", action=\"store_true\", help=\"Whether to run training.\")\n",
        "    parser.add_argument(\"--do_eval\", action=\"store_true\", help=\"Whether to run eval on the dev set.\")\n",
        "    parser.add_argument(\n",
        "        \"--evaluate_during_training\", action=\"store_true\", help=\"Run evaluation during training at each logging step.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--do_lower_case\", action=\"store_true\", help=\"Set this flag if you are using an uncased model.\"\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\"--per_gpu_train_batch_size\", default=8, type=int, help=\"Batch size per GPU/CPU for training.\")\n",
        "    parser.add_argument(\n",
        "        \"--per_gpu_eval_batch_size\", default=8, type=int, help=\"Batch size per GPU/CPU for evaluation.\"\n",
        "    )\n",
        "    parser.add_argument(\"--learning_rate\", default=5e-5, type=float, help=\"The initial learning rate for Adam.\")\n",
        "    parser.add_argument(\n",
        "        \"--gradient_accumulation_steps\",\n",
        "        type=int,\n",
        "        default=1,\n",
        "        help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n",
        "    )\n",
        "    parser.add_argument(\"--weight_decay\", default=0.0, type=float, help=\"Weight decay if we apply some.\")\n",
        "    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n",
        "    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n",
        "    parser.add_argument(\n",
        "        \"--num_train_epochs\", default=3.0, type=float, help=\"Total number of training epochs to perform.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--max_steps\",\n",
        "        default=-1,\n",
        "        type=int,\n",
        "        help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\",\n",
        "    )\n",
        "    parser.add_argument(\"--warmup_steps\", default=0, type=int, help=\"Linear warmup over warmup_steps.\")\n",
        "    parser.add_argument(\n",
        "        \"--n_best_size\",\n",
        "        default=20,\n",
        "        type=int,\n",
        "        help=\"The total number of n-best predictions to generate in the nbest_predictions.json output file.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--max_answer_length\",\n",
        "        default=30,\n",
        "        type=int,\n",
        "        help=\"The maximum length of an answer that can be generated. This is needed because the start \"\n",
        "        \"and end predictions are not conditioned on one another.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--verbose_logging\",\n",
        "        action=\"store_true\",\n",
        "        help=\"If true, all of the warnings related to data processing will be printed. \"\n",
        "        \"A number of warnings are expected for a normal SQuAD evaluation.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--lang_id\",\n",
        "        default=0,\n",
        "        type=int,\n",
        "        help=\"language id of input for language-specific xlm models (see tokenization_xlm.PRETRAINED_INIT_CONFIGURATION)\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\"--logging_steps\", type=int, default=500, help=\"Log every X updates steps.\")\n",
        "    parser.add_argument(\"--save_steps\", type=int, default=500, help=\"Save checkpoint every X updates steps.\")\n",
        "    parser.add_argument(\n",
        "        \"--eval_all_checkpoints\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number\",\n",
        "    )\n",
        "    parser.add_argument(\"--no_cuda\", action=\"store_true\", help=\"Whether not to use CUDA when available\")\n",
        "    parser.add_argument(\n",
        "        \"--overwrite_output_dir\", action=\"store_true\", help=\"Overwrite the content of the output directory\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--overwrite_cache\", action=\"store_true\", help=\"Overwrite the cached training and evaluation sets\"\n",
        "    )\n",
        "    parser.add_argument(\"--seed\", type=int, default=42, help=\"random seed for initialization\")\n",
        "\n",
        "    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"local_rank for distributed training on gpus\")\n",
        "    parser.add_argument(\n",
        "        \"--fp16\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--fp16_opt_level\",\n",
        "        type=str,\n",
        "        default=\"O1\",\n",
        "        help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
        "        \"See details at https://nvidia.github.io/apex/amp.html\",\n",
        "    )\n",
        "    parser.add_argument(\"--server_ip\", type=str, default=\"\", help=\"Can be used for distant debugging.\")\n",
        "    parser.add_argument(\"--server_port\", type=str, default=\"\", help=\"Can be used for distant debugging.\")\n",
        "\n",
        "    parser.add_argument(\"--threads\", type=int, default=1, help=\"multiple threads for converting example to features\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if args.doc_stride >= args.max_seq_length - args.max_query_length:\n",
        "        logger.warning(\n",
        "            \"WARNING - You've set a doc stride which may be superior to the document length in some \"\n",
        "            \"examples. This could result in errors when building features from the examples. Please reduce the doc \"\n",
        "            \"stride or increase the maximum length to ensure the features are correctly built.\"\n",
        "        )\n",
        "\n",
        "    if (\n",
        "        os.path.exists(args.output_dir)\n",
        "        and os.listdir(args.output_dir)\n",
        "        and args.do_train\n",
        "        and not args.overwrite_output_dir\n",
        "    ):\n",
        "        raise ValueError(\n",
        "            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n",
        "                args.output_dir\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # Setup distant debugging if needed\n",
        "    if args.server_ip and args.server_port:\n",
        "        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n",
        "        import ptvsd\n",
        "\n",
        "        print(\"Waiting for debugger attach\")\n",
        "        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n",
        "        ptvsd.wait_for_attach()\n",
        "\n",
        "    # Setup CUDA, GPU & distributed training\n",
        "    if args.local_rank == -1 or args.no_cuda:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
        "        args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n",
        "    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
        "        torch.cuda.set_device(args.local_rank)\n",
        "        device = torch.device(\"cuda\", args.local_rank)\n",
        "        torch.distributed.init_process_group(backend=\"nccl\")\n",
        "        args.n_gpu = 1\n",
        "    args.device = device\n",
        "\n",
        "    # Setup logging\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n",
        "    )\n",
        "    logger.warning(\n",
        "        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
        "        args.local_rank,\n",
        "        device,\n",
        "        args.n_gpu,\n",
        "        bool(args.local_rank != -1),\n",
        "        args.fp16,\n",
        "    )\n",
        "\n",
        "    # Set seed\n",
        "    set_seed(args)\n",
        "\n",
        "    # Load pretrained model and tokenizer\n",
        "    if args.local_rank not in [-1, 0]:\n",
        "        # Make sure only the first process in distributed training will download model & vocab\n",
        "        torch.distributed.barrier()\n",
        "\n",
        "    args.model_type = args.model_type.lower()\n",
        "    config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
        "    config = config_class.from_pretrained(\n",
        "        args.config_name if args.config_name else args.model_name_or_path,\n",
        "        cache_dir=args.cache_dir if args.cache_dir else None,\n",
        "    )\n",
        "    tokenizer = tokenizer_class.from_pretrained(\n",
        "        args.tokenizer_name if args.tokenizer_name else args.model_name_or_path,\n",
        "        do_lower_case=args.do_lower_case,\n",
        "        cache_dir=args.cache_dir if args.cache_dir else None,\n",
        "    )\n",
        "    model = model_class.from_pretrained(\n",
        "        args.model_name_or_path,\n",
        "        from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
        "        config=config,\n",
        "        cache_dir=args.cache_dir if args.cache_dir else None,\n",
        "    )\n",
        "\n",
        "    if args.local_rank == 0:\n",
        "        # Make sure only the first process in distributed training will download model & vocab\n",
        "        torch.distributed.barrier()\n",
        "\n",
        "    model.to(args.device)\n",
        "\n",
        "    logger.info(\"Training/evaluation parameters %s\", args)\n",
        "\n",
        "    # Before we do anything with models, we want to ensure that we get fp16 execution of torch.einsum if args.fp16 is set.\n",
        "    # Otherwise it'll default to \"promote\" mode, and we'll get fp32 operations. Note that running `--fp16_opt_level=\"O2\"` will\n",
        "    # remove the need for this code, but it is still valid.\n",
        "    if args.fp16:\n",
        "        try:\n",
        "            import apex\n",
        "\n",
        "            apex.amp.register_half_function(torch, \"einsum\")\n",
        "        except ImportError:\n",
        "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
        "\n",
        "    # Training\n",
        "    if args.do_train:\n",
        "        train_dataset = load_and_cache_examples(args, tokenizer, evaluate=False, output_examples=False)\n",
        "        global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n",
        "        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
        "\n",
        "    # Save the trained model and the tokenizer\n",
        "    if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n",
        "        # Create output directory if needed\n",
        "        if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:\n",
        "            os.makedirs(args.output_dir)\n",
        "\n",
        "        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n",
        "        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "        # They can then be reloaded using `from_pretrained()`\n",
        "        # Take care of distributed/parallel training\n",
        "        model_to_save = model.module if hasattr(model, \"module\") else model\n",
        "        model_to_save.save_pretrained(args.output_dir)\n",
        "        tokenizer.save_pretrained(args.output_dir)\n",
        "\n",
        "        # Good practice: save your training arguments together with the trained model\n",
        "        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n",
        "\n",
        "        # Load a trained model and vocabulary that you have fine-tuned\n",
        "        model = model_class.from_pretrained(args.output_dir)  # , force_download=True)\n",
        "        tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n",
        "        model.to(args.device)\n",
        "\n",
        "    # Evaluation - we can ask to evaluate all the checkpoints (sub-directories) in a directory\n",
        "    results = {}\n",
        "    if args.do_eval and args.local_rank in [-1, 0]:\n",
        "        if args.do_train:\n",
        "            logger.info(\"Loading checkpoints saved during training for evaluation\")\n",
        "            checkpoints = [args.output_dir]\n",
        "            if args.eval_all_checkpoints:\n",
        "                checkpoints = list(\n",
        "                    os.path.dirname(c)\n",
        "                    for c in sorted(glob.glob(args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True))\n",
        "                )\n",
        "                logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce model loading logs\n",
        "        else:\n",
        "            logger.info(\"Loading checkpoint %s for evaluation\", args.model_name_or_path)\n",
        "            checkpoints = [args.model_name_or_path]\n",
        "\n",
        "        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
        "\n",
        "        for checkpoint in checkpoints:\n",
        "            # Reload the model\n",
        "            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n",
        "            model = model_class.from_pretrained(checkpoint)  # , force_download=True)\n",
        "            model.to(args.device)\n",
        "\n",
        "            # Evaluate\n",
        "            result = evaluate(args, model, tokenizer, prefix=global_step)\n",
        "\n",
        "            result = dict((k + (\"_{}\".format(global_step) if global_step else \"\"), v) for k, v in result.items())\n",
        "            results.update(result)\n",
        "\n",
        "    logger.info(\"Results: {}\".format(results))\n",
        "\n",
        "    return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5tCKIY0t0ib",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4Hv9Sv7Ln94",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8AIiYlmt25n",
        "colab_type": "code",
        "outputId": "51143728-671f-4baa-eb2c-ddc5e522cdff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python CSE576/transformers-master/examples/run_squad.py --model_type bert --do_eval --model_name_or_path bert-base-uncased   --do_lower_case --train_file CSE576/data/Output_updated/bert_train.json --predict_file CSE576/data/Output_updated/bert_dev.json  --per_gpu_train_batch_size 16 --learning_rate 3e-5 --num_train_epochs 2.0  --max_seq_length 128  --gradient_accumulation_steps 2 --doc_stride 32 --output_dir CSE576/Data/Output/output_bertbase_clicr_seqlen128_temp --logging_steps 5000 --save_steps 5000"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "03/23/2020 20:31:17 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "03/23/2020 20:31:17 - INFO - filelock -   Lock 140330487085936 acquired on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685.lock\n",
            "03/23/2020 20:31:17 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp_ukdh9rt\n",
            "Downloading: 100% 361/361 [00:00<00:00, 258kB/s]\n",
            "03/23/2020 20:31:18 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json in cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
            "03/23/2020 20:31:18 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
            "03/23/2020 20:31:18 - INFO - filelock -   Lock 140330487085936 released on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685.lock\n",
            "03/23/2020 20:31:18 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
            "03/23/2020 20:31:18 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_ids\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "03/23/2020 20:31:18 - INFO - filelock -   Lock 140330113805224 acquired on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
            "03/23/2020 20:31:18 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp2rqea3m7\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 931kB/s] \n",
            "03/23/2020 20:31:19 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt in cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "03/23/2020 20:31:19 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "03/23/2020 20:31:19 - INFO - filelock -   Lock 140330113805224 released on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
            "03/23/2020 20:31:19 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "03/23/2020 20:31:19 - INFO - filelock -   Lock 140330113744568 acquired on /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock\n",
            "03/23/2020 20:31:19 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp0u7keqtt\n",
            "Downloading: 100% 440M/440M [00:16<00:00, 26.7MB/s]\n",
            "03/23/2020 20:31:36 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin in cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "03/23/2020 20:31:36 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "03/23/2020 20:31:36 - INFO - filelock -   Lock 140330113744568 released on /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock\n",
            "03/23/2020 20:31:36 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "03/23/2020 20:31:39 - INFO - transformers.modeling_utils -   Weights of BertForQuestionAnswering not initialized from pretrained model: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "03/23/2020 20:31:39 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "03/23/2020 20:31:43 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir=None, device=device(type='cuda'), do_eval=True, do_lower_case=True, do_train=False, doc_stride=32, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=2, lang_id=0, learning_rate=3e-05, local_rank=-1, logging_steps=5000, max_answer_length=30, max_grad_norm=1.0, max_query_length=64, max_seq_length=128, max_steps=-1, model_name_or_path='bert-base-uncased', model_type='bert', n_best_size=20, n_gpu=1, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=2.0, output_dir='CSE576/Data/Output/output_bertbase_clicr_seqlen128_temp', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=16, predict_file='CSE576/data/Output_updated/bert_dev.json', save_steps=5000, seed=42, server_ip='', server_port='', threads=1, tokenizer_name='', train_file='CSE576/data/Output_updated/bert_train.json', verbose_logging=False, version_2_with_negative=False, warmup_steps=0, weight_decay=0.0)\n",
            "03/23/2020 20:31:43 - INFO - __main__ -   Loading checkpoint bert-base-uncased for evaluation\n",
            "03/23/2020 20:31:43 - INFO - __main__ -   Evaluate the following checkpoints: ['bert-base-uncased']\n",
            "03/23/2020 20:31:43 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
            "03/23/2020 20:31:43 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_ids\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "03/23/2020 20:31:44 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "03/23/2020 20:31:47 - INFO - transformers.modeling_utils -   Weights of BertForQuestionAnswering not initialized from pretrained model: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "03/23/2020 20:31:47 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "03/23/2020 20:31:47 - INFO - __main__ -   Loading features from cached file ./cached_dev_bert-base-uncased_128\n",
            "03/23/2020 20:31:52 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "03/23/2020 20:31:52 - INFO - __main__ -     Num examples = 32888\n",
            "03/23/2020 20:31:52 - INFO - __main__ -     Batch size = 8\n",
            "Evaluating: 100% 4111/4111 [02:28<00:00, 27.69it/s]\n",
            "03/23/2020 20:34:20 - INFO - __main__ -     Evaluation done in total 148.450094 secs (0.004514 sec per example)\n",
            "03/23/2020 20:34:20 - INFO - transformers.data.metrics.squad_metrics -   Writing predictions to: CSE576/Data/Output/output_bertbase_clicr_seqlen128_temp/predictions_.json\n",
            "03/23/2020 20:34:20 - INFO - transformers.data.metrics.squad_metrics -   Writing nbest to: CSE576/Data/Output/output_bertbase_clicr_seqlen128_temp/nbest_predictions_.json\n",
            "OrderedDict([('bcr-2012-006996.1', 'more persistent deformity , or at the exocortical surface , with no effect on'), ('bcr-2012-006996.2', 'more persistent deformity , or at the exocortical surface , with no effect on'), ('bcr-2012-006996.4', 'more persistent deformity , or at the exocortical surface , with no effect on'), ('bcr-2012-006996.5', 'more persistent deformity , or at the exocortical surface , with no effect on'), ('bcr-2012-006996.6', 'more persistent deformity , or at the exocortical surface , with no effect on'), ('bcr-2012-006996.8', 'more persistent deformity , or at the exocortical surface , with no effect on'), ('bcr-2012-006996.13', 'more persistent deformity , or at the exocortical surface , with no effect on'), ('bcr-2013-010177.1', 'character and was not associated with radiation to any site . There was no history of fever ,'), ('bcr-2013-010177.2', 'expectoration of pink frothy sput'), ('bcr-2013-010177.3', 'expectoration of pink frothy sput'), ('bcr-2013-010177.4', 'contralateral lung . Treatment is primarily supportive , with the administration of supplemental oxygen and diuretics , and endotrac'), ('bcr-2013-010177.5', 'fibrillation ensued'), ('bcr-2013-010177.7', 'endotrac'), ('bcr-2012-006642.1', 'and information derived from breast US and MRI scans to ascertain the possibility of a primary tumour within the normal breast . If any'), ('bcr-2012-006642.2', 'and information derived from breast US and MRI scans to ascertain the possibility of a primary tumour within the normal breast . If any'), ('bcr-2013-008788.1', 'restrictive cardiomyopathy and constrictive pericarditis . Ultimately , tissue may'), ('bcr-2013-008788.2', 'constrictive pericarditis . Ultimately , tissue may'), ('bcr-2013-008788.4', 'restrictive cardiomyopathy and constrictive pericarditis . Ultimately , tissue may'), ('bcr-2013-008788.5', 'thickened pericardium encasing the heart . Background This patient did'), ('bcr-2013-008788.7', 'restrictive cardiomyopathy and constrictive pericarditis . Ultimately , tissue may'), ('bcr-2014-206913.2', 'less significant in secondary forms of RCVS . The exact pathogenesis of RCVS remains unclear ; however , sympathetic over activity may have a'), ('bcr-2014-206913.3', 'role to play'), ('bcr-2014-206913.5', 'role to play'), ('bcr-2014-206913.9', 'vessels but the exposure to radiation limits its use . The CSF is usually normal , but it may show slight abnormalities such as pl'), ('bcr-2014-206913.10', 'forms of RCVS . The exact pathogenesis of RCVS remains unclear ; however , sympathetic over activity may have a'), ('bcr-2014-206913.14', 'intracerebral vessels but the exposure to radiation limits its use . The CSF is usually normal , but it may show slight abnormalities such as pl'), ('bcr-2014-206913.16', 'role to play'), ('bcr-2013-202914.1', 'Neuroglial tissue with'), ('bcr-2013-202914.2', 'Neuroglial tissue with'), ('bcr-2013-202914.3', 'Neuroglial tissue with'), ('bcr-2013-202914.4', 'Neuroglial tissue with'), ('bcr-2013-202914.7', 'with hair foll'), ('bcr-2013-202914.8', 'with hair foll'), ('bcr-2015-211884.2', 'flank pain associated with gross haematuria of 10 days duration . There was no history of trauma , surgical intervention or any'), ('bcr-2015-211884.4', 'flank pain associated with gross haematuria of 10 days duration . There was no history of trauma , surgical intervention or any'), ('bcr-2015-211884.9', 'flank or abdominal pain with or'), ('bcr-2013-203318.3', 'and the RARA gene on chromosome 17 . Juxtap'), ('bcr-2013-203318.5', 'and the RARA gene on chromosome 17 . Juxtap'), ('bcr-2013-203318.9', 'and the RARA gene on chromosome 17 . Juxtap'), ('bcr-2013-201462.1', 'likely . Subendocardial sparing is not seen in myocardial infarction , and the affected areas do'), ('bcr-2013-201462.2', 'gadolinium during cardiac MRI is a'), ('bcr-2013-201462.4', 'gadolinium during cardiac MRI is a useful modality for detecting cardiac sarcoidosis2 and can'), ('bcr-2013-201462.6', 'gadolinium during cardiac MRI is a useful modality for detecting cardiac sarcoidosis2 and can'), ('bcr.08.2011.4575.2', 'with its concomitant complications and a postoperative hospital stay of 10 – 14 days . The large incision carries the'), ('bcr.08.2011.4575.3', 'with its concomitant complications and a postoperative hospital stay of 10 – 14 days . The large incision carries the'), ('bcr.08.2011.4575.5', 'with its concomitant complications and a postoperative hospital stay of 10 – 14 days . The large incision carries the'), ('bcr.08.2011.4575.6', 'transperitoneal approach . The patient was subjected to general anaesthesia ; a nasogastric tube and urethral catheter'), ('bcr.08.2011.4575.8', 'with its concomitant complications and a postoperative hospital stay of 10 – 14 days . The large incision carries the'), ('bcr-2016-214807.1', 'and humeral ‘ defor'), ('bcr-2016-214807.5', 'and shoulder joint movement . Shoulder X-rays revealed deformit'), ('bcr-2016-215445.1', 'fields . This pattern is usually a pulmonary manifestation of idiopathic inflammatory myopathy , which may'), ('bcr-2016-215445.2', 'radiological appearance . Case presentation A 54 - year - old HIV negative man , a farmer , was referred to our'), ('bcr-2016-215445.3', 'fields . This pattern is usually a pulmonary manifestation of idiopathic inflammatory myopathy , which may'), ('bcr-2016-215445.4', 'clinicopathological entity characterised by presence of buds of gran'), ('bcr-2016-215445.7', 'fields . This pattern is usually a pulmonary manifestation of idiopathic inflammatory myopathy , which may'), ('bcr-2016-215445.8', 'fields . This pattern is usually a pulmonary manifestation of idiopathic inflammatory myopathy , which may'), ('bcr-2016-215445.9', 'fields . This pattern is usually a pulmonary manifestation of idiopathic inflammatory myopathy , which may'), ('bcr-2016-215445.10', 'fields . This pattern is usually a pulmonary manifestation of idiopathic inflammatory myopathy , which may'), ('bcr-2016-214994.3', 'bronchomalacia is the commonest cause ; however , as in our patient , a'), ('bcr-2016-214994.4', 'bronchomalacia is the commonest cause ; however , as in our patient , a definitive cause is not found in up'), ('bcr-2016-214994.5', 'acute presentation . Furthermore , the patient ’ s weight gain during this period was inadequate , which should have'), ('bcr-2016-214994.6', 'have'), ('bcr-2016-214994.7', 'bronchomalacia is the commonest cause ; however , as in our patient , a'), ('bcr-2016-214994.8', 'acute presentation . Furthermore , the patient ’ s weight gain during this period was inadequate , which should have'), ('bcr-2016-214994.9', '; to distinguish CLE from pneumothorax ; and to avoid positive pressure at induction of anaesthesia . Children with CLE face a'), ('bcr.12.2011.5307.1', 'trichloroacetic acid ( TCA ) . Because lesions that appear to be hidrocystomas could be malignant or coexist'), ('bcr.12.2011.5307.2', 'trichloroacetic acid ( TCA ) . Because lesions that appear to be hidrocystomas could be malignant or coexist'), ('bcr.12.2011.5307.3', 'trichloroacetic acid ( TCA ) . Because lesions that appear to be hidrocystomas could be malignant or coexist'), ('bcr.12.2011.5307.5', 'trichloroacetic acid ( TCA ) . Because lesions that appear to be hidrocystomas could be malignant or coexist'), ('bcr.12.2011.5307.6', 'tumour could have'), ('bcr.12.2011.5307.9', 'TCA ) . Because lesions that appear to be hidrocystomas could be malignant or coe'), ('bcr.12.2011.5307.12', 'trichloroacetic acid ( TCA ) . Because lesions that appear to be hidrocystomas could be malignant or coexist'), ('bcr.12.2011.5307.13', 'trichloroacetic acid ( TCA ) . Because lesions that appear to be hidrocystomas could be malignant or coexist'), ('bcr-2015-213819.1', 'gadolinium enhancement on MRI as this may have'), ('bcr-2015-213819.2', 'gadolinium enhancement on MRI as this may have'), ('bcr-2015-213819.3', 'gadolinium enhancement on MRI as this may have'), ('bcr-2015-213819.4', 'gadolinium enhancement on MRI as this may have'), ('bcr-2015-213819.5', 'gadolinium enhancement on MRI as this may have'), ('bcr-2015-213819.6', 'gadolinium enhancement on MRI as this may have'), ('bcr-2015-213819.7', 'for replacement fibrosis in women with late gadolinium enhancement on MRI as this may have'), ('bcr-2015-213819.9', 'for replacement fibrosis in women with late gadolinium enhancement on MRI as this may have'), ('bcr-2015-209925.3', 'chemotherapeutic agents in patients with previously high sun exposure . Such cutaneous manifestations do'), ('bcr-2015-209925.7', 'for breast'), ('bcr-2015-209925.8', 'chemotherapeutic agents in patients with previously high sun exposure . Such cutaneous manifestations do'), ('bcr-2014-208212.1', 'and livedo reticularis who was diagnosed with Streptococcus acidominimus endocarditis . To the best of our'), ('bcr-2014-208212.2', 'pansystolic murmur . Laboratory evaluation found neutrophil leucocytosis ; elevated C reactive protein and blood cultures grew Streptococcus'), ('bcr-2014-208212.3', 'and livedo reticularis who was diagnosed with Streptococcus acidominimus endocarditis . To the best of our'), ('bcr-2014-208212.4', 'and livedo reticularis who was diagnosed with Streptococcus acidominimus endocarditis . To the best of our'), ('bcr-2014-208212.5', 'pansystolic murmur . Laboratory evaluation found neutrophil leucocytosis ; elevated C reactive protein and blood cultures grew Streptococcus'), ('bcr-2014-208212.6', 'pansystolic murmur . Laboratory evaluation found neutrophil leucocytosis ; elevated C reactive protein and blood cultures grew Streptococcus'), ('bcr-2014-208212.8', 'and livedo reticularis who was diagnosed with Streptococcus acidominimus endocarditis . To the best of our'), ('bcr-2014-208212.9', 'and livedo reticularis who was diagnosed with Streptococcus acidominimus endocarditis . To the best of our'), ('bcr-2014-208212.11', 'and livedo reticularis who was diagnosed with Streptococcus acidominimus endocarditis . To the best of our'), ('bcr-2014-208212.12', 'and mild dementia . The patient lived in a residential home and did not smoke or'), ('bcr-2014-208212.13', 'and mild dementia . The patient lived in a residential home and did not smoke or'), ('bcr.06.2009.2007.8', 'trige'), ('bcr.06.2009.2007.9', 'trige'), ('bcr.06.2009.2007.10', 'somnolence , paraesthaesia , diminished appetite , nausea , diarrhoea , weight loss , abdominal pain and dysgeusia'), ('bcr.11.2008.1219.1', 'and benign ovarian pathology . BACKGROUND Discerning the cause of ascites can be'), ('bcr.11.2008.1219.3', 'and benign ovarian pathology . BACKGROUND Discerning the cause of ascites can be'), ('bcr.11.2008.1219.4', 'and benign ovarian pathology . BACKGROUND Discerning the cause of ascites can be'), ('bcr-2013-009979.1', 'atenolol 50 mg once daily presented to casualty with retrosternal'), ('bcr-2013-009979.2', 'atenolol 50 mg once daily presented to casualty with retrosternal'), ('bcr-2013-009979.3', 'atenolol 50 mg once daily presented to casualty with retrosternal'), ('bcr-2013-009979.5', 'atenolol 50 mg once daily presented to casualty with retrosternal'), ('bcr-2013-009979.8', 'atenolol 50 mg once daily presented to casualty with retrosternal'), ('bcr-2014-205723.1', 'outlined ( white arrows ) . Also seen are hyperdensities along the inferior margin of corpus call'), ('bcr-2014-205723.3', 'outlined ( white arrows ) . Also seen are hyperdensities along the inferior margin of corpus call'), ('bcr-2014-205723.5', 'outlined ( white arrows ) . Also seen are hyperdensities along the inferior margin of corpus call'), ('bcr-2014-205723.8', 'veins during the process possibly has'), ('bcr-2014-205723.9', 'white arrow ) . Cerebral air embolism is usually seen as small millimetric hypodens'), ('bcr-2014-205723.10', 'veins during the process possibly has'), ('bcr-2014-205723.11', 'veins during the process possibly has'), ('bcr-2014-205723.12', 'veins during the process possibly has'), ('bcr.01.2012.5644.2', 'term corticosteroid co-therapy . Other than her advanced age , our patient did not have'), ('bcr.01.2012.5644.3', 'term corticosteroid co-therapy . Other'), ('bcr.01.2012.5644.4', 'seated infection caused by mucosal commensal flora such as Parvimonas as an emerging problem in these patients . Furthermore ,'), ('bcr.01.2012.5644.5', 'seated infection caused by mucosal commensal flora such as Parvimonas as an emerging problem in these patients . Furthermore ,'), ('bcr.01.2012.5644.7', 'term corticosteroid co-therapy . Other than her advanced age , our patient did not have'), ('bcr.01.2012.5644.8', '-TNFα therapy leads to profound suppressive effects on the innate immune response . A study by Strangfeld et al showed that a'), ('bcr.01.2012.5644.9', 'term corticosteroid co-therapy . Other'), ('bcr.01.2012.5644.10', 'etanercept ) 50 mg once weekly for the past 6 years . She does'), ('bcr.03.2009.1687.2', 'and more peripheral erythaema . Such lesions were described as being call'), ('bcr.03.2009.1687.3', 'and more peripheral erythaema . Such lesions were described as being call'), ('bcr.03.2009.1687.6', 'and more peripheral erythaema . Such lesions were described as being call'), ('bcr.03.2009.1687.7', 'and more peripheral erythaema . Such lesions were described as being call'), ('bcr.03.2009.1687.8', 'and more peripheral erythaema . Such lesions were described as being call'), ('bcr.03.2009.1687.10', 'and more peripheral erythaema . Such lesions were described as being call'), ('bcr.03.2009.1687.11', 'more peripheral erythaema . Such lesions were described as being call'), ('bcr.03.2009.1687.14', 'and more peripheral erythaema . Such lesions were described as being call'), ('bcr.03.2009.1687.15', 'and more peripheral erythaema . Such lesions were described as being call'), ('bcr.03.2009.1687.16', 'more peripheral erythaema . Such lesions were described as being call'), ('bcr.03.2009.1687.17', 'and more peripheral erythaema . Such lesions were described as being call'), ('bcr.03.2009.1687.19', 'and more peripheral erythaema . Such lesions were described as being call'), ('bcr.03.2009.1687.20', 'and more peripheral erythaema . Such lesions were described as being call'), ('bcr.03.2009.1687.21', 'and more peripheral erythaema . Such lesions were described as being call'), ('bcr-2013-202196.1', 'tumour lysis syndrome was likely to have'), ('bcr-2013-202196.2', 'ischaemia . Cytokines secreted by the lymphoma cells may also play a'), ('bcr-2013-202196.4', 'tumour lysis syndrome was likely to have'), ('bcr.08.2011.4635.2', 'benign prostatic hyperplasia . Rarely , however , inhibitors may act on'), ('bcr-2014-208680.2', ', tinea nigra and maduromycosis ( eg , Mad'), ('bcr.10.2011.5028.1', 'paris backslab with the ankle in a neutral position . Her'), ('bcr.10.2011.5028.2', 'and foot region , with no associated neurovascular compromise . She was sent for foot radiographs , at this stage , to delineate any'), ('bcr.10.2011.5028.5', 'and foot region , with no associated neurovascular compromise . She was sent for foot radiographs , at this stage , to delineate any'), ('bcr.10.2011.5028.7', 'no bony nor physeal injury . However , her foot was increasingly swollen and bruised during her'), ('bcr.10.2011.5028.8', 'and foot region , with no associated neurovascular compromise . She was sent for foot radiographs , at this stage , to delineate any'), ('bcr.10.2011.5028.9', 'and foot region , with no associated neurovascular compromise . She was sent for foot radiographs , at this stage , to delineate any'), ('bcr-2015-214166.1', 'infiltrative type , depending on the structure the cells differentiate into . The variants that show a differentiation towards hair structures are called'), ('bcr-2015-214166.4', 'infiltrative type , depending on the structure the cells differentiate into . The variants that show a differentiation towards hair structures are called'), ('bcr-2015-214166.5', 'tumours and has'), ('bcr-2015-214166.9', 'infiltrative type , depending on the structure the cells differentiate into . The variants that show a differentiation towards hair structures are called'), ('bcr-2015-214166.10', 'infiltrative type , depending on the structure the cells differentiate into . The variants that show a differentiation towards hair structures are called'), ('bcr-2014-204625.2', 'eruption in a clearly demarcated distribution could be explained by a shower of microemboli passing through the rete acromialis . The'), ('bcr-2014-204625.3', 'arterial network ( the re'), ('bcr-2014-204625.4', 'arterial network ( the'), ('bcr-2014-204625.5', 'arterial network ( the'), ('bcr-2014-204625.6', 'arterial network ( the re'), ('bcr-2014-204625.7', 'eruption in a clearly demarcated distribution could be'), ('bcr-2014-204625.8', 'arterial network ( the re'), ('bcr-2014-204928.2', 'mesothelial surface of the peritoneum . In adults , it lies underneath the subserosal stroma'), ('bcr-2014-204928.3', 'mesothelial surface of the peritoneum . In adults , it lies underneath the subserosal stroma'), ('bcr-2014-204928.5', 'vascularised by the omental vessels ( figure 2 ) . Histology confirmed both to be lei'), ('bcr-2014-204928.6', 'vascularised by the omental vessels ( figure 2 ) . Histology confirmed both to be lei'), ('bcr-2014-204928.8', 'mesothelial surface of the peritoneum . In adults , it lies underneath the subserosal stroma'), ('bcr-2014-204928.9', 'mesothelial surface of the peritoneum . In adults , it lies underneath the subserosal stroma'), ('bcr-2014-204928.10', 'mesothelial surface of the peritoneum . In adults , it lies underneath the subserosal stroma'), ('bcr-2013-203361.2', 'polymyositis . The conclusion of probable carcinomatosis polyradiculitis or'), ('bcr-2013-203361.3', 'and distention . She underwent a right salpingo - oophorect'), ('bcr-2013-203361.4', 'and distention . She underwent a right salpingo - oophorect'), ('bcr-2013-202212.2', 'IOP ) . Recognition of associations , complications and differentiating features help us decide appropriate surgeries in such cases thereby ensuring'), ('bcr-2013-202212.4', 'curvature . Humphrey visual field was unreliable in the left eye . Differential diagnosis The enlarged thick cornea and associated iris , angle and posterior segment anomalies'), ('bcr-2013-202212.5', 'curvature . Humphrey visual field was unreliable in the left eye . Differential diagnosis The enlarged thick cornea and associated iris , angle and posterior segment anomalies'), ('bcr-2013-202212.7', 'posterior segment stretching ( seen as chorioretinal atrophy or my'), ('bcr-2013-202212.12', 'subconjunctival application for 3 min ) in the left eye ( figure 2 ) with no intraoperative or'), ('bcr-2013-202212.13', 'ciliary band in both eyes . Fundus examination showed bilateral posteriorly dislocated cat'), ('bcr.06.2008.0033.2', 'did'), ('bcr.06.2008.0033.4', 'did'), ('bcr.06.2008.0033.5', 'did'), ('bcr-2013-011036.4', 'endothelial injury . VLST has been reported in the coronary stent literature , although it is rare . The discontinuation or'), ('bcr-2013-011036.5', 'endothelial injury . VLST has been reported in the coronary stent literature , although it is rare . The discontinuation or'), ('bcr-2013-011036.6', 'endothelial injury . VLST has been reported in the coronary stent literature , although it is rare . The discontinuation or'), ('bcr-2013-011036.7', 'endothelial injury . VLST has been reported in the coronary stent literature , although it is rare . The discontinuation or'), ('bcr-2013-011036.12', 'hybrid combinations , respectively , due to the free cells and independent segments of the open - cell design . The captured floating'), ('bcr-2013-011036.13', 'endothelial injury . VLST has been reported in the coronary stent literature , although it is rare . The discontinuation or'), ('bcr-2013-011036.15', 'denser than those in the right side . There was no in - stent stenosis on'), ('bcr.05.2010.3039.1', 'fluid emboli14 does'), ('bcr.05.2010.3039.2', 'fluid emboli14 does'), ('bcr.05.2010.3039.3', 'fluid emboli14 does'), ('bcr.05.2010.3039.6', 'fluid emboli14 does'), ('bcr.05.2010.3039.7', 'fluid emboli14 does'), ('bcr.05.2010.3039.8', 'fluid emboli14 does'), ('bcr.05.2010.3039.9', 'fluid emboli14 does'), ('bcr.05.2010.3039.10', 'fluid emboli14 does'), ('bcr.05.2010.3039.11', 'fluid emboli14 does'), ('bcr.05.2010.3039.12', 'fluid emboli14 does'), ('bcr.05.2010.3039.13', 'fluid emboli14 does'), ('bcr.05.2010.3039.14', 'fluid emboli14 does'), ('bcr.01.2012.5563.1', \"a patient 's symptoms , especially in certain groups of individuals . ‘ At risk ’ groups include athletes , military recruits or anybody who has made a\"), ('bcr.01.2012.5563.3', 'and he was treated with oral anti-inflammatory agents . His discomfort was felt to be consistent with his current level of training and'), ('bcr.01.2012.5563.4', 'of intense physical training or following the commencement of new or'), ('bcr.01.2012.5563.6', 'any'), ('bcr.01.2012.5563.7', 'and he was treated with oral anti-inflammatory agents . His discomfort was felt to be consistent with his current level of training and'), ('bcr.06.2011.4304.1', 'haematological spread from a distant focus of infection . While not considered at the time , the patient does'), ('bcr.06.2011.4304.3', 'and endocarditis . The authors were only able to find two case reports on PubMed relating to this subject . Given this lack'), ('bcr.06.2011.4304.7', 'haematological spread from a distant focus of infection . While not considered at the time , the patient does'), ('bcr.06.2011.4304.9', 'haematological spread from a distant focus of infection . While not considered at the time , the patient does'), ('bcr.06.2011.4304.10', 'PubMed relating to this subject . Given this lack of clear causal evidence , we are unable to state with conviction that'), ('bcr.06.2011.4304.13', 'haematological spread from a distant focus of infection . While not considered at the time , the patient does'), ('bcr.06.2011.4304.14', 'haematological spread from a distant focus of infection . While not considered at the time , the patient does'), ('bcr.06.2011.4304.15', 'haematological spread from a distant focus of infection . While not considered at the time , the patient does'), ('bcr.06.2011.4304.16', 'haematological spread from a distant focus of infection . While not considered at the time , the patient does'), ('bcr.11.2010.3539.2', 'although rare . 5 , – , 8 The importance of early detection lies'), ('bcr.11.2010.3539.4', 'antitubercular treatment . TB should'), ('bcr.11.2010.3539.6', 'although rare . 5 , – , 8 The importance of early detection lies'), ('bcr.10.2010.3421.1', 'column sensations without'), ('bcr.10.2010.3421.3', 'column sensations without'), ('bcr.10.2010.3421.5', 'column sensations without'), ('bcr.11.2008.1296.2', 'Vasculitis induced injury to blood vessels may lead to increased vascular permeability , vessel weakening that causes aneurysm formation or'), ('bcr.11.2008.1296.4', 'Vasculitis induced injury to blood vessels may lead to increased vascular permeability , vessel weakening that causes aneurysm formation or'), ('bcr.11.2008.1296.5', 'Vasculitis induced injury to blood vessels may lead to increased vascular permeability , vessel weakening that causes aneurysm formation or'), ('bcr.11.2008.1296.6', 'Vasculitis shows a predilection for the small bowel but thromboembolism may'), ('bcr.11.2008.1296.8', 'Vasculitis shows a predilection for the small bowel but thromboembolism may'), ('bcr.11.2008.1296.10', 'polyarteritis nodosa'), ('bcr-2012-008518.1', 'non-bilious vomit . There was no history of fever , constipation or'), ('bcr-2012-008518.2', 'non-bilious vomit . There was no history of fever , constipation or'), ('bcr-2012-008518.3', 'non-bilious vomit . There was no history of fever , constipation or'), ('bcr-2012-008518.4', 'non-bilious vomit . There was no history of fever , constipation or'), ('bcr-2012-008518.5', 'non-bilious vomit . There was no history of fever , constipation or'), ('bcr-2012-008518.6', 'non-bilious vomit . There was no history of fever , constipation or'), ('bcr-2012-008518.7', 'non-bilious vomit . There was no history of fever , constipation or'), ('bcr-2012-008518.9', 'non-bilious vomit . There was no history of fever , constipation or'), ('bcr.10.2008.1163.4', 'subfoveal lesions OU ( fig 1A ) . Wearing her full cycloplegic refraction , the patient had'), ('bcr.10.2008.1163.5', 'column are for the left eye and right tracings for the right eye . Stimulus intens'), ('bcr.10.2008.1163.8', 'subfoveal lesions OU ( fig 1A ) . Wearing her full cycloplegic refraction , the patient had'), ('bcr-2013-009008.2', 'venous extension or'), ('bcr-2013-009008.5', 'venous extension or'), ('bcr-2013-009008.6', 'venous extension or'), ('bcr-2013-009008.7', 'venous extension or'), ('bcr-2013-009008.8', 'venous extension or'), ('bcr-2013-009008.9', 'venous extension or'), ('bcr-2013-009008.11', 'vascularity which would'), ('bcr-2012-007756.1', 'adherent to the treatment . She was detected ser'), ('bcr-2012-007756.2', 'tumour ; with almost complete recovery . Background Immunocompromised status in AIDS makes the differential diagnosis of any'), ('bcr-2012-007756.3', 'way to rifampicin but to a lesser extent . Maintaining adequate anticoag'), ('bcr-2012-007756.4', 'tumour ; with almost complete recovery . Background Immunocompromised status in AIDS makes the differential diagnosis of any'), ('bcr.07.2009.2091.1', 'of vitiligo . To date , no report has provided a detailed description of how MG treatment can'), ('bcr.07.2009.2091.2', 'vulgaris , the immunological factors associated with MG may have'), ('bcr.07.2009.2091.5', 'of vitiligo . To date , no report has provided a detailed description of how MG treatment can'), ('bcr-2013-201311.1', 'and reduced osteoblast and osteocyte viability . This cu'), ('bcr-2013-201311.2', 'extended effect ? Clinically , should'), ('bcr-2013-201311.4', 'blood vessels . The net effect is reduced water content of bone and reduced osteoblast and osteocyte viability . This cu'), ('bcr-2013-201311.6', 'Sangle et al2 are forming during periods of suboptimal anticoagulation . Microfractures may'), ('bcr-2013-201311.8', 'Sangle et al2 are forming during periods of suboptimal anticoagulation . Microfractures may'), ('bcr-2013-201311.9', 'Sangle et al2 are forming during periods of suboptimal anticoagulation . Microfractures may'), ('bcr-2014-204631.1', 'normal ( figure 2 ) . Blood picture showed lymphocytosis , raised erythrocyte sediment'), ('bcr-2014-204631.6', 'autoimmune disorder of the central nervous system ( CNS ) . Antiviral antibodies or a'), ('bcr-2014-204631.11', 'Fundus ( LE ) was normal ( figure 2 ) . Blood picture showed lymphocytosis , raised erythrocyte sediment'), ('bcr.04.2010.2896.2', 'which was reported in our patient , is composed of large macroscopic lymphatic spaces surrounded by collagen and smooth muscles and does'), ('bcr.04.2010.2896.3', 'and her'), ('bcr.04.2010.2896.4', 'and her'), ('bcr.04.2010.2896.6', 'and her'), ('bcr.04.2010.2896.7', 'smooth muscles and does not have'), ('bcr.04.2010.2896.8', 'flank and another 5 × 6 cm cystic lesion seen in the pelvis with'), ('bcr.04.2010.2896.10', 'smooth muscles and does not have'), ('bcr.04.2010.2896.11', 'which was reported in our patient , is composed of large macroscopic lymphatic spaces surrounded by collagen and smooth muscles and does'), ('bcr.04.2010.2896.12', 'which was reported in our patient , is composed of large macroscopic lymphatic spaces surrounded by collagen and smooth muscles and does'), ('bcr.04.2010.2896.13', 'which was reported in our patient , is composed of large macroscopic lymphatic spaces surrounded by collagen and smooth muscles and does'), ('bcr.04.2010.2896.14', 'which was reported in our patient , is composed of large macroscopic lymphatic spaces surrounded by collagen and smooth muscles and does'), ('bcr-2014-205100.1', 'with or'), ('bcr-2014-205100.3', '- based chemotherapy regimens to a vitamin A derivative , the all - trans retinoic acid , with or'), ('bcr-2014-205100.4', '- based chemotherapy regimens to a vitamin A derivative , the all - trans retinoic acid , with or'), ('bcr-2014-205100.5', '- based chemotherapy regimens to a vitamin A derivative , the all - trans retinoic acid , with or'), ('bcr-2014-205100.6', '- based chemotherapy regimens to a vitamin A derivative , the all - trans retinoic acid , with or'), ('bcr-2014-205100.7', '- based chemotherapy regimens to a vitamin A derivative , the all - trans retinoic acid , with or'), ('bcr-2014-205100.11', 'with or'), ('bcr-2014-205100.13', 'with or'), ('bcr-2014-205100.14', 'with or'), ('bcr-2014-205100.15', 'with or'), ('bcr.03.2009.1645.1', 'cryptogenic stroke and may be associated with paradoxical emboli to the brain . 12 Before performing the TOE to diagnose whether or'), ('bcr.03.2009.1645.2', 'cryptogenic stroke and may be associated with paradoxical emboli to the brain . 12 Before performing the TOE to diagnose whether or'), ('bcr.03.2009.1645.4', 'cryptogenic stroke and may be associated with paradoxical emboli to the brain . 12 Before performing the TOE to diagnose whether or'), ('bcr.03.2009.1645.5', 'cryptogenic stroke and may be associated with paradoxical emboli to the brain . 12 Before performing the TOE to diagnose whether or'), ('bcr.03.2009.1645.6', 'cryptogenic stroke and may be associated with paradoxical emboli to the brain . 12 Before performing the TOE to diagnose whether or'), ('bcr.03.2009.1645.7', 'cryptogenic stroke and may be associated with paradoxical emboli to the brain . 12 Before performing the TOE to diagnose whether or'), ('bcr.03.2009.1645.8', 'cryptogenic stroke and may be associated with paradoxical emboli to the brain . 12 Before performing the TOE to diagnose whether or'), ('bcr.03.2009.1645.9', 'cryptogenic stroke and may be associated with paradoxical emboli to the brain . 12 Before performing the TOE to diagnose whether or'), ('bcr.03.2009.1645.10', 'cryptogenic stroke and may be associated with paradoxical emboli to the brain . 12 Before performing the TOE to diagnose whether or'), ('bcr.03.2009.1645.11', 'cryptogenic stroke and may be associated with paradoxical emboli to the brain . 12 Before performing the TOE to diagnose whether or'), ('bcr.03.2009.1645.12', 'cryptogenic stroke and may be associated with paradoxical emboli to the brain . 12 Before performing the TOE to diagnose whether or'), ('bcr.03.2009.1645.14', 'cryptogenic stroke and may be associated with paradoxical emboli to the brain . 12 Before performing the TOE to diagnose whether or'), ('bcr.03.2009.1645.15', 'cryptogenic stroke and may be associated with paradoxical emboli to the brain . 12 Before performing the TOE to diagnose whether or'), ('bcr.03.2009.1645.16', 'cryptogenic stroke and may be associated with paradoxical emboli to the brain . 12 Before performing the TOE to diagnose whether or'), ('bcr.03.2009.1645.17', 'cryptogenic stroke and may be associated with paradoxical emboli to the brain . 12 Before performing the TOE to diagnose whether or'), ('bcr.03.2009.1645.18', 'cryptogenic stroke and may be associated with paradoxical emboli to the brain . 12 Before performing the TOE to diagnose whether or'), ('bcr.03.2009.1645.20', 'cryptogenic stroke and may be associated with paradoxical emboli to the brain . 12 Before performing the TOE to diagnose whether or'), ('bcr.03.2009.1645.21', 'cryptogenic stroke and may be associated with paradoxical emboli to the brain . 12 Before performing the TOE to diagnose whether or'), ('bcr.05.2011.4275.1', 'tumour , and therefore clear margins were achieved . There was no evidence of local or'), ('bcr.05.2011.4275.2', 'duodenal fistula . At laparotomy it was found that a 6.2 cm gallstone had fist'), ('bcr.05.2011.4275.5', 'tumour , and therefore clear margins were achieved . There was no evidence of local or'), ('bcr.05.2011.4275.6', 'Bouveret ’ s syndrome . Differential diagnosis Treatment After further resuscitation , laparotomy was undertaken which found a gallbladder empyema'), ('bcr.05.2011.4275.7', 'Bouveret ’ s syndrome . Differential diagnosis Treatment After further resuscitation , laparotomy was undertaken which found a gallbladder empyema'), ('bcr.05.2011.4275.11', 'duodenal fistula . At laparotomy it was found that a 6.2 cm gallstone had fist'), ('bcr.05.2011.4275.14', 'duodenal fistula . At laparotomy it was found that a 6.2 cm gallstone had fist'), ('bcr.05.2011.4275.15', 'duodenal fistula . At laparotomy it was found that a 6.2 cm gallstone had fist'), ('bcr.05.2011.4275.18', 'duodenal fist'), ('bcr-2013-201293.3', 'beneficial and takes'), ('bcr-2013-201293.4', 'beneficial and takes'), ('bcr-2013-201293.5', 'immunosuppression would have taken atleast couple of weeks for its action to appear . IOP raised on day 1 was unusual in our'), ('bcr-2013-201293.7', 'immunosuppression would have taken atleast couple of weeks for its action to appear . IOP raised on day 1 was unusual in our'), ('bcr.10.2011.4935.1', 'Sweet ’ s syndrome may sign'), ('bcr.10.2011.4935.2', 'and myelodysplasia , four subsequently developed RP . The coexistence of Sweet ’ s syndrome and RP is'), ('bcr.10.2011.4935.3', 'and myelodysplasia , four subsequently developed RP . The coexistence of Sweet ’ s syndrome and RP is'), ('bcr.10.2011.4935.4', 'Sweet ’ s syndrome in patients with RP and myelodysplasia may'), ('bcr.10.2011.4935.5', 'Sweet ’ s syndrome in patients with RP and myelodysplasia may'), ('bcr.10.2011.4935.6', 'malignant transformation associated with antitumour necrosis factor α ( TNFα ) agents . To our knowledge , this is'), ('bcr.10.2011.4935.10', 'malignant transformation associated with antitumour necrosis factor α ( TNFα ) agents . To our knowledge , this is'), ('bcr.10.2011.4935.11', 'malignant transformation associated with antitumour necrosis factor α ( TNFα ) agents . To our knowledge , this is'), ('bcr.10.2011.4935.13', 'Sweet ’ s syndrome and RP is therefore likely to represent a true association , rather than the coincidental'), ('bcr.08.2011.4679.1', 'and other cognitive and behavioural disturbances . Usually affecting adolescent males , episodes normally last up to a'), ('bcr-2013-009111.2', 'and hyponatraemia , since sodium homeostasis is intact . The volume expansion activates'), ('bcr-2013-009111.4', 'and hyponatraemia , since sodium homeostasis is intact . The volume expansion activates'), ('bcr-2013-009111.8', 'and hyponatraemia , since sodium homeostasis is intact . The volume expansion activates'), ('bcr-2013-202773.1', 'and early surgical debride'), ('bcr-2013-202773.3', 'and early surgical debridement for improved outcomes . Prompt surgical debride'), ('bcr-2013-202773.4', 'and early surgical debridement for improved outcomes . Prompt surgical debride'), ('bcr-2013-202773.9', 'and early surgical debridement for improved outcomes . Prompt surgical debride'), ('bcr-2012-007671.2', 'conditions . Our case adds'), ('bcr-2012-007671.14', 'postpyloric feeding with an elemental formula via a nasojejunal'), ('bcr-2012-007671.17', 'conditions . Our case adds'), ('bcr-2012-007671.18', 'conditions . Our case adds'), ('bcr-2012-007671.19', 'conditions . Our case adds'), ('bcr-2015-213333.1', 'with the other pituitary hormonal axes being normal does not seem to justify'), ('bcr-2015-213333.5', 'with the other pituitary hormonal axes being normal does not seem to justify'), ('bcr-2015-213333.6', 'with the other pituitary hormonal axes being normal does not seem to justify'), ('bcr-2012-006656.1', 'abdomen such as intussusception . 2 Indeed , the diverticulum often has'), ('bcr-2012-006656.2', 'abdomen such as intussusception . 2 Indeed , the diverticulum often has'), ('bcr-2012-006656.3', 'abdomen such as intussusception . 2 Indeed , the diverticulum often has'), ('bcr-2012-006656.4', 'abdomen such as intussusception . 2 Indeed , the diverticulum often has'), ('bcr-2012-006656.5', 'abdomen such as intussusception . 2 Indeed , the diverticulum often has'), ('bcr-2012-006656.6', 'abdomen such as intussusception . 2 Indeed , the diverticulum often has'), ('bcr-2013-201742.1', '/ NK cell lymphoma . To the best of our knowledge , this is'), ('bcr-2013-201742.3', '/ NK cell lymphoma . To the best of our knowledge , this is'), ('bcr-2013-201742.4', '/ NK cell lymphoma . To the best of our knowledge , this is'), ('bcr-2013-201742.5', 'haematem'), ('bcr-2013-201742.6', 'haematem'), ('bcr-2013-201742.7', 'haematem'), ('bcr-2015-212022.4', 'with a'), ('bcr-2015-212022.5', 'and the second was an isolated case of coronoid hypoplasia . Thus it can be'), ('bcr-2015-212022.6', 'endothelial'), ('bcr-2015-212022.11', 'a'), ('bcr-2014-205261.1', 'tumour ( arrow head ) . ( A ) Cells in the primary tumour exhibit a high nuclear / cytoplasmic ratio and form a'), ('bcr-2014-205261.2', 'tumour ( arrow head ) . ( A ) Cells in the primary tumour exhibit a high nuclear / cytoplasmic ratio and form a'), ('bcr-2014-205261.3', 'tumour ( arrow head ) . ( A ) Cells in the primary tumour exhibit a high nuclear / cytoplasmic ratio and form a'), ('bcr-2014-205261.5', 'malignant transformation . Each subgroup of HCA has a'), ('bcr-2014-205261.9', 'malignant transformation of HCA . Case presentation A 72 - year - old man was referred to our'), ('bcr-2014-206475.1', 'fluid management and clear any endotoxins . Despite fluid resus'), ('bcr-2014-206475.3', 'histology . 14 Granulomas may be visualised on CT as multiple discrete , low attenuation , non-enhancing nodules'), ('bcr-2014-206475.7', 'histology . 14 Granulomas may be visualised on CT as multiple discrete , low attenuation , non-enhancing nodules'), ('bcr-2014-206475.8', 'histology . 14 Granulomas may be visualised on CT as multiple discrete , low attenuation , non-enhancing nodules'), ('bcr-2014-206475.9', 'fibrosis in histology . 14 Granulomas may be visualised on CT as multiple discrete , low attenuation , non-enhancing nod'), ('bcr-2014-206475.10', 'acidosis was caused iatrogenically by metformin , although this indeed could have'), ('bcr-2014-206475.11', 'fibrosis in histology . 14 Granulomas may be visualised on CT as multiple discrete , low attenuation , non-enhancing nod'), ('bcr-2014-206475.12', 'acidosis was caused iatrogenically by metformin , although this indeed could have'), ('bcr-2014-206102.1', 'overflexed in frontal injury and overextended in parietal lesions . We postulate'), ('bcr-2014-206102.2', 'overextended in parietal lesions . We postulate'), ('bcr-2014-206102.5', 'overextended in parietal lesions . We postulate'), ('bcr-2014-206102.6', 'overflexed in frontal injury and overextended in parietal lesions . We postulate'), ('bcr-2014-206102.7', 'overextended in parietal lesions . We postulate'), ('bcr-2014-206102.8', 'overflexed in frontal injury and overextended in parietal lesions . We postulate'), ('bcr-2014-206102.9', 'overextended in parietal lesions . We postulate that the distinctive presenting symptom in this case , alternating hemidystonia'), ('bcr-2014-206102.12', 'with decerebrate / decorticate dystonic posturing . Episodes presented with autonomic dysregul'), ('bcr-2014-206102.13', 'with decerebrate / decorticate dystonic posturing . Episodes presented with autonomic dysregul'), ('bcr-2014-206102.15', 'with decerebrate / decorticate dystonic posturing . Episodes presented with autonomic dysregul'), ('bcr-2014-207778.2', 'arthropathy . 21 In this patient , it is difficult to establish whether some of the abnormalities identified are related to the patient ’ s'), ('bcr-2014-207778.3', '’ s disease remains a diagnosis of exclusion . 16 Diagnostic criteria are based on clinical features and not on'), ('bcr-2014-207778.4', 'arthropathy . 21 In this patient , it is difficult to establish whether some of the abnormalities identified are related to the patient ’ s'), ('bcr-2014-207778.5', 'arthropathy . 21 In this patient , it is difficult to establish whether some of the abnormalities identified are related to the patient ’ s'), ('bcr-2014-207778.6', 'arthropathy . 21 In this patient , it is difficult to establish whether some of the abnormalities identified are related to the patient ’ s'), ('bcr-2014-207778.10', 'and symptoms may be treated even though'), ('bcr-2014-207778.11', 'and symptoms may be treated even though'), ('bcr-2014-207778.12', 'and symptoms may be treated even though'), ('bcr-2014-207778.13', 'and symptoms may be treated even though'), ('bcr-2014-207778.15', 'arthropathy . 21 In this patient , it is difficult to establish whether some of the abnormalities identified are related to the patient ’ s'), ('bcr-2014-207778.16', 'arthropathy . 21 In this patient , it is difficult to establish whether some of the abnormalities identified are related to the patient ’ s'), ('bcr-2014-207778.17', 'and symptoms may be treated even though'), ('bcr-2014-207778.18', 'arthropathy . 21 In this patient , it is difficult to establish whether some of the abnormalities identified are related to the patient ’ s'), ('bcr-2014-207778.21', '’ s disease remains a diagnosis of exclusion . 16 Diagnostic criteria are based on clinical features and not on'), ('bcr-2014-207778.22', '’ s disease remains a diagnosis of exclusion . 16 Diagnostic criteria are based on clinical features and not on'), ('bcr-2014-207778.23', 'overlap syndrome is present with a'), ('bcr-2014-207778.24', '’ s disease remains a diagnosis of exclusion . 16 Diagnostic criteria are based on clinical features and not on'), ('bcr-2014-207778.25', 'arthropathy . 21 In this patient , it is difficult to establish whether some of the abnormalities identified are related to the patient ’ s'), ('bcr-2014-207778.26', 'arthropathy . 21 In this patient , it is difficult to establish whether some of the abnormalities identified are related to the patient ’ s'), ('bcr-2014-206609.4', 'and DHT . There are over 300 different mutations that can induce this syndrome . The degree of insensitivity determines'), ('bcr-2014-206609.5', 'ambiguous genitalia ) to minor AIS ( infertile male ) . It is important to emphasise that these patients do'), ('bcr-2014-206609.7', 'androgen receptor ( AR ) is not fully sensitive for testosterone and DHT . There are over 300 different mutations that can'), ('bcr-2014-206609.8', 'androgen receptor ( AR ) is not fully sensitive for testosterone and DHT . There are over 300 different mutations that can'), ('bcr.06.2011.4309.1', 'spinal cord and lodged in the anterior column . The C4 and'), ('bcr.06.2011.4309.2', 'spinal cord and lodged in the anterior column . The C4 and 5 spinous processes and lam'), ('bcr.06.2011.4309.6', 'column . The C4 and'), ('bcr.06.2011.4309.7', 'spinal cord and lodged in the anterior column . The C4 and'), ('bcr.06.2011.4309.8', 'column . The C4 and'), ('bcr.06.2011.4309.10', 'spinal cord lesion caused by hemise'), ('bcr.06.2011.4309.12', 'cord contus'), ('bcr.06.2011.4309.13', 'cord contus'), ('bcr.06.2011.4309.14', 'cord contus'), ('bcr-2014-207804.1', 'vasculitis in adults and is commoner in the elderly . The superficial temporal artery is the commonest'), ('bcr-2014-207804.3', 'vasculitis in adults and is commoner in the elderly . The superficial temporal artery is the commonest'), ('bcr-2014-207804.9', 'vasculitis in adults and is commoner in the elderly . The superficial temporal artery is'), ('bcr-2014-207147.1', 'condyle . The skin paddle of the graft was folded into'), ('bcr-2014-207147.2', 'vascular tissue suitable for reconstruction of the nasal lining that is easily harvested with acceptable donor site deformity . The free radial forearm flap will'), ('bcr-2014-207147.3', 'condyle . The skin paddle of the graft was folded into a'), ('bcr-2014-207147.4', 'condyle . The skin paddle of the graft was folded into'), ('bcr-2014-207147.6', 'condyle . The skin paddle of the graft was folded into'), ('bcr-2014-207147.7', 'vascular tissue suitable for reconstruction of the nasal lining that is easily harvested with acceptable donor site deformity . The free radial forearm flap will'), ('bcr-2014-207147.8', 'vascular tissue suitable for reconstruction of the nasal lining that is easily harvested with acceptable donor site deformity . The free radial forearm flap will'), ('bcr-2014-209153.1', 'Calciphylaxis'), ('bcr-2014-209153.2', 'calciphylaxis treated with sodium thiosulfate in seven dialysis centres . Data showed a 52 % complete remission rate . Remission depended'), ('bcr-2014-209153.3', 'fibrillation ; therefore , continuous anticoagulation'), ('bcr-2014-209153.4', 'calciphyactic lesions . The patient was taken to the operating room for penile debridement'), ('bcr-2014-209153.5', 'calciphyactic lesions . The patient was taken to the operating room for penile debridement'), ('bcr-2014-209153.6', 'fibrillation ; therefore , continuous anticoagulation'), ('bcr-2014-209153.7', 'fibrillation ; therefore , continuous anticoagulation'), ('bcr-2014-209153.8', 'Calciphylaxis'), ('bcr-2012-008464.1', '14 Bilateral , severe renal artery stenosis stimulates the renin - angiotensin aldoster'), ('bcr-2012-008464.3', 'renovascular hypertension : medical antihypertensive therapy ,'), ('bcr-2012-008464.4', 'renovascular hypertension : medical antihypertensive therapy ,'), ('bcr-2012-008464.7', 'Bilateral , severe renal artery stenosis stimulates the renin - angiotensin aldoster'), ('bcr-2012-008464.9', 'anatomy or in those who require pararenal aortic reconstructions for aortic aneurysms or severe aort'), ('bcr-2012-008464.10', 'anatomy or in those who require pararenal aortic reconstructions for aortic aneurysms or severe aort'), ('bcr-2014-207420.1', 'fjord - like ’ , irregular outlines and tr'), ('bcr-2014-207420.3', 'fjord - like ’ , irregular outlines and tr'), ('bcr-2016-215702.1', 'nod'), ('bcr-2016-215702.4', 'nod'), ('bcr-2016-215702.6', 'nod'), ('bcr-2016-215702.17', 'nod'), ('bcr-2016-215702.20', 'nod'), ('bcr-2015-212813.13', 'and wear an orthotic insole to reduce leg length discre'), ('bcr-2015-212813.14', 'and pain . A detailed history was taken and there was no suspicion of physical'), ('bcr-2015-212813.15', 'and pain . A detailed history was taken and there was no suspicion of physical'), ('bcr-2015-212813.16', 'and wear an orthotic insole to reduce leg length discre'), ('bcr-2016-214721.1', \"fibrin aggregates throughout multiple samples from this patient 's transbronchial biopsy is not a\"), ('bcr-2016-214721.3', 'lavage also did not identify an infectious aetiology — nor did'), ('bcr-2016-214721.4', 'lavage also did not identify an infectious aetiology — nor did'), ('bcr-2016-214721.5', 'transbronchial biopsy is not a'), ('bcr-2016-214721.6', 'transbronchial biopsy is not a'), ('bcr-2016-214721.7', 'productive cough , dyspnoea , pleuritic or subster'), ('bcr-2016-214721.8', 'transbronchial biopsy is not a'), ('bcr-2016-214721.10', 'transbronchial biopsy is not a'), ('bcr-2016-214721.12', 'lavage also did not identify an infectious aetiology — nor did'), ('bcr-2016-214721.13', 'aetiology — nor did'), ('bcr-2016-214721.14', 'for bleomycin - induced lung damage was'), ('bcr-2013-201459.6', 'and enhancing double wall at places , and a small'), ('bcr-2013-201459.7', 'and enhancing double wall at places , and a small'), ('bcr-2013-201459.8', 'and enhancing double wall at places , and a small'), ('bcr-2013-201459.12', 'and enhancing double wall at places , and a small'), ('bcr-2013-201459.15', 'and enhancing double wall at places , and a small'), ('bcr.11.2009.2486.1', ', trichuriasis and tapeworm disease . Isopods are the most diverse in form and the most species - rich crust'), ('bcr.11.2009.2486.3', ', trichuriasis and tapeworm disease . Isopods are the most diverse in form and the most species - rich crust'), ('bcr.11.2009.2486.5', ', trichuriasis and tapeworm disease . Isopods are the most diverse in form and the most species - rich crust'), ('bcr.11.2009.2486.6', 'adhesions , followed by her'), ('bcr.11.2009.2486.7', 'and tapeworm disease . Isopods are the most diverse in form and the most species - rich crust'), ('bcr.11.2009.2486.8', 'adhesions , followed by her'), ('bcr.11.2009.2486.11', 'and tapeworm disease . Isopods are the most diverse in form and the most species - rich crust'), ('bcr.11.2009.2486.13', 'and tapeworm disease . Isopods are the most diverse in form and the most species - rich crust'), ('bcr.11.2009.2486.17', ', anisa'), ('bcr.11.2009.2486.18', 'and tapeworm disease . Isopods are the most diverse in form and the most species - rich crust'), ('bcr.11.2009.2486.21', 'and tapeworm disease . Isopods are the most diverse in form and the most species - rich crust'), ('bcr.11.2009.2486.25', 'and tapeworm disease . Isopods are the most diverse in form and the most species - rich crust'), ('bcr-2015-211376.1', 'complex matter . It is important to note that sepsis was not suspected in this patient . Oral antibiotic treatment would never have'), ('bcr-2015-211376.2', 'complex matter . It is important to note that sepsis was not suspected in this patient . Oral antibiotic treatment would never have'), ('bcr-2015-211376.4', 'prevalence of osteomyelitis , these simple and non-expensive investigations can be used to establish or'), ('bcr-2015-211376.5', 'microbiological agent in our patient . We combined oral antibiotics with locally administered gentamicin and this could have'), ('bcr-2015-211376.8', 'microbiological agent in our patient . We combined oral antibiotics with locally administered gentamicin and this could have'), ('bcr-2015-211376.10', 'prevalence of osteomyelitis , these simple and non-expensive investigations can be used to establish or'), ('bcr-2015-211376.12', 'prevalence of osteomyelitis , these simple and non-expensive investigations can be used to establish or'), ('bcr-2013-200858.1', 'scrofulosorum is a rare form of tuberculid seen in children and young adults with or'), ('bcr-2013-200858.2', 'scrofulosorum is a rare form of tuberculid seen in children and young adults with or'), ('bcr-2013-200858.4', 'scrofulosorum is a rare form of tuberculid seen in children and young adults with or'), ('bcr-2013-200858.5', 'scrofulosorum is a rare form of tuberculid seen in children and young adults with or'), ('bcr-2013-200858.6', 'scrofulosorum is a rare form of tuberculid seen in children and young adults with or'), ('bcr-2013-200858.7', 'misdiagnosed . Lichen scrofulosorum is a rare form of tuberculid seen in children and young adults with or'), ('bcr-2013-200858.8', 'scrofulosorum is a rare form of tuberculid seen in children and young adults with or'), ('bcr-2013-200858.9', 'tuberculous infection . Key features of tuberculids include a strongly positive tuberculin skin test , evidence of concomitant manifest or'), ('bcr-2013-200858.10', 'scrofulosorum is a rare form of tuberculid seen in children and young adults with or'), ('bcr-2013-200858.12', 'scrofulosorum is a rare form of tuberculid seen in children and young adults with or'), ('bcr.08.2010.3271.1', 'duodenoscopy revealed two columns of small oes'), ('bcr.08.2010.3271.2', 'lymphocytes , total protein 2.21 g / dl and albumin 1.2 g / dl . There were no pus cells , organisms or'), ('bcr.08.2010.3271.3', 'duodenoscopy revealed two columns of small oes'), ('bcr.08.2010.3271.4', 'conservative treatment is about 55 % . Either somatostatin or octre'), ('bcr.08.2010.3271.6', 'conservative treatment is about 55 % . Either somatostatin or octre'), ('bcr.11.2008.1179.2', 'vascularity of the radiation damaged tissue7 , 8 However , due to the site and location'), ('bcr.11.2008.1179.5', 'vascularity of the radiation damaged tissue7 , 8 However , due to the site and location'), ('bcr.11.2008.1179.9', 'vascularity of the radiation damaged tissue7 , 8 However , due to the site and location'), ('bcr.11.2008.1179.10', 'vascularity of the radiation damaged tissue7 , 8 However , due to the site and location'), ('bcr.11.2008.1179.11', 'vascularity of the radiation damaged tissue7 , 8 However , due to the site and location'), ('bcr.11.2008.1179.13', 'meningitis . ( fig 3 ) This was confirmed on cerebrospinal'), ('bcr.11.2008.1179.15', 'superimposed infection of osteoradionecrotic bone complicated by cranial extension with pneumocephalus , meningitis'), ('bcr-2016-215527.1', 'willingness of both parties to carry out the act successfully . 12 However , we can not rule out'), ('bcr.05.2011.4269.3', 'vein patching . A direct injury to the limb carrying a thrombosed AVF should be examined and evidence of embolisation warrants'), ('bcr.05.2011.4269.6', 'Arteriovenous fistulae ( AVF ) are commonly required for dialysis prior to renal transplantation , and are subsequently left insit')])\n",
            "03/23/2020 20:34:33 - INFO - __main__ -   Results: {'exact': 0.2, 'f1': 0.7162221507422744, 'total': 500, 'HasAns_exact': 0.2, 'HasAns_f1': 0.7162221507422744, 'HasAns_total': 500, 'best_exact': 0.2, 'best_exact_thresh': 0.0, 'best_f1': 0.7162221507422744, 'best_f1_thresh': 0.0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsSZ8KJcDCTT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}